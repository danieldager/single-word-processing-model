{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to sample 50,000 words: 0.0004 seconds\n",
      "First 10 sampled words: ['the', 'the', 'whereas', 'play', 'the', 'more', 'to', 'possible', 'inbound', 'at']\n",
      "\n",
      "Top 10 most frequently sampled words:\n",
      "the: 29 times (true frequency: 0.053700)\n",
      "and: 19 times (true frequency: 0.025700)\n",
      "to: 17 times (true frequency: 0.026900)\n",
      "of: 13 times (true frequency: 0.025100)\n",
      "is: 7 times (true frequency: 0.011700)\n",
      "in: 7 times (true frequency: 0.018600)\n",
      "a: 7 times (true frequency: 0.022900)\n",
      "on: 7 times (true frequency: 0.008130)\n",
      "so: 6 times (true frequency: 0.003310)\n",
      "for: 5 times (true frequency: 0.010200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from wordfreq import iter_wordlist, word_frequency\n",
    "import time\n",
    "\n",
    "class FastWordSampler:\n",
    "    def __init__(self, language='en', max_words=100000):\n",
    "        self.words = []\n",
    "        self.language = language\n",
    "        self.cumulative_probs = []\n",
    "        \n",
    "        total_freq = 0\n",
    "        for i, word in enumerate(iter_wordlist(language)):\n",
    "            if i >= max_words:\n",
    "                break\n",
    "            freq = word_frequency(word, language)\n",
    "            total_freq += freq\n",
    "            self.words.append(word)\n",
    "            self.cumulative_probs.append(total_freq)\n",
    "        \n",
    "        self.cumulative_probs = np.array(self.cumulative_probs) / total_freq\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        random_values = np.random.random(n_samples)\n",
    "        indices = np.searchsorted(self.cumulative_probs, random_values)\n",
    "        return [self.words[i] for i in indices]\n",
    "\n",
    "sampler = FastWordSampler()\n",
    "samples = sampler.sample(500)\n",
    "\n",
    "# Optional: Check frequencies of sampled words\n",
    "# from collections import Counter\n",
    "\n",
    "# word_counts = Counter(samples)\n",
    "# print(\"\\nTop 10 most frequently sampled words:\")\n",
    "# for word, count in word_counts.most_common(10):\n",
    "#     print(f\"{word}: {count} times (true frequency: {word_frequency(word, 'en'):.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 sampled words and their phonemes:\n",
      "the: DH AH0\n",
      "the: DH AH0\n",
      "whereas: W EH0 R AE1 Z\n",
      "play: P L EY1\n",
      "the: DH AH0\n",
      "more: M AO1 R\n",
      "to: T UW1\n",
      "possible: P AA1 S AH0 B AH0 L\n",
      "inbound: IH0 N B AW1 N D\n",
      "at: AE1 T\n"
     ]
    }
   ],
   "source": [
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "words_and_phonemes = [(word, g2p(word)) for word in samples]\n",
    "\n",
    "print(\"\\nFirst 10 sampled words and their phonemes:\")\n",
    "for word, phonemes in words_and_phonemes[:10]:\n",
    "    print(f\"{word}: {' '.join(phonemes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: websites\n",
      "Shape of one-hot encoded tensor: torch.Size([14, 60])\n",
      "One-hot encoded tensor:\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary to map phonemes to indices\n",
    "phoneme_to_index = defaultdict(lambda: len(phoneme_to_index) + 1)\n",
    "\n",
    "# Function to encode a single word's phonemes\n",
    "def encode_word(phonemes):\n",
    "    return [phoneme_to_index[p] for p in phonemes]\n",
    "\n",
    "# Encode all phonemes from each word in the data\n",
    "encoded_phonemes = [torch.tensor(encode_word(phonemes)) for _, phonemes in words_and_phonemes]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "padded_sequences = pad_sequence(encoded_phonemes, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create one-hot encodings\n",
    "vocab_size = len(phoneme_to_index) + 1\n",
    "one_hot_encoded = torch.nn.functional.one_hot(padded_sequences, num_classes=vocab_size).float()\n",
    "\n",
    "# Create a list of (word, one-hot encoded tensor) pairs\n",
    "encoded_data = [(word, one_hot) for (word, _), one_hot in zip(words_and_phonemes, one_hot_encoded)]\n",
    "\n",
    "# Print the result for the first word\n",
    "print(f\"Word: {encoded_data[100][0]}\")\n",
    "print(f\"Shape of one-hot encoded tensor: {encoded_data[100][1].shape}\")\n",
    "print(f\"One-hot encoded tensor:\\n{encoded_data[100][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Separate words and encodings\n",
    "words, encodings = zip(*encoded_data)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(torch.stack(list(encodings)))\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs = batch[0]\n",
    "#     print(f\"Batch shape: {inputs.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def text_to_image_tensor(\n",
    "        words: list=[\"text\"], savepath=None, index=1, mirror=False,\n",
    "        fontname='Arial', W = 64, H = 64, size=10, spacing=0,\n",
    "        xshift=0, yshift=-3, upper=False, invert=False, show=None\n",
    "    ):\n",
    "\n",
    "    tensors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if upper: word = word.upper()\n",
    "        if invert: word = word[::-1]\n",
    "        \n",
    "        img = Image.new(\"L\", (W,H), color=10)\n",
    "        fnt = ImageFont.truetype(fontname+'.ttf', size)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Starting word anchor\n",
    "        w = sum([(fnt.getbbox(l)[2] - fnt.getbbox(l)[0]) for l in word])\n",
    "        h = sum([(fnt.getbbox(l)[3] - fnt.getbbox(l)[1]) for l in word]) / len(word)\n",
    "        w = w + spacing * (len(word) - 1)\n",
    "        h_anchor = (W - w) / 2\n",
    "        v_anchor = (H - h) / 2\n",
    "\n",
    "        x, y = (xshift + h_anchor, yshift + v_anchor)\n",
    "        \n",
    "        for l in word:\n",
    "            draw.text((x,y), l, font=fnt, fill=\"white\")\n",
    "            letter_w = fnt.getbbox(l)[2] - fnt.getbbox(l)[0]\n",
    "            x += letter_w + spacing\n",
    "\n",
    "        if x > (W + spacing + 2) or (xshift + h_anchor) < -1:\n",
    "            raise ValueError(f\"Text width is bigger than image. Failed on size:{size}\")\n",
    "        \n",
    "        if savepath:\n",
    "            img.save(f\"{savepath}/{word}.jpg\")\n",
    "\n",
    "        img_np = np.array(img)\n",
    "        img_tensor = torch.from_numpy(img_np)\n",
    "        tensors.append((word, img_tensor))\n",
    "    \n",
    "    return tensors\n",
    "\n",
    "tensors = text_to_image_tensor(samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 sampled words and their image tensors:\n",
      "the: torch.Size([64, 64])\n",
      "the: torch.Size([64, 64])\n",
      "whereas: torch.Size([64, 64])\n",
      "play: torch.Size([64, 64])\n",
      "the: torch.Size([64, 64])\n",
      "more: torch.Size([64, 64])\n",
      "to: torch.Size([64, 64])\n",
      "possible: torch.Size([64, 64])\n",
      "inbound: torch.Size([64, 64])\n",
      "at: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 10 sampled words and their image tensors:\")\n",
    "for word, tensor in tensors:\n",
    "    print(f\"{word}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
